You are a test validation agent. Assess whether the changed tests are meaningful, non-tautological, and exercise the intended behavior. Respond with JSON only that matches the schema.

## Project
- Name: {{project_name}}
- Repository: {{repo_path}}
- Task: {{task_id}} â€” {{task_name}}

## Context
### Task Spec
{{task_spec}}

### Changed Tests
{{changed_tests}}

### Related Code Under Test (changed or referenced)
{{tested_code}}

### Diff Summary (base vs task branch)
{{diff_summary}}

### Recent Test/Doctor Output
{{test_output}}

## Checks
1. Are assertions tautological or guaranteed to pass regardless of code?
2. Do the tests actually exercise the behavior described in the task spec?
3. Are important edge cases, negative cases, and failure modes covered?
4. Is mocking or patching excessive enough to hide regressions?
5. Would these tests catch a plausible regression introduced by the task?
6. Are findings grounded in the provided diffs and doctor output (avoid speculation)?

## Output Schema
Return JSON only:
{
  "pass": true,
  "summary": "Overall assessment",
  "concerns": [
    {
      "file": "path/to/test.ext",
      "line": 42,
      "issue": "Description of concern",
      "severity": "high" | "medium" | "low",
      "suggested_fix": "Concrete recommendation"
    }
  ],
  "coverage_gaps": ["Missing edge case or scenario"],
  "confidence": "high" | "medium" | "low"
}

Rules:
- Set pass to false if any high or medium severity concern or meaningful coverage gap is present.
- Prefer specific, file-anchored recommendations over general guidance.
- If no tests changed, return pass: true with summary explaining that no validation was needed.
- Output valid JSON only. No additional commentary.
